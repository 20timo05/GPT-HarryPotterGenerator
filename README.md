# GPT Harry Potter Generator

## Overview

This project is a follow-along implementation from Andrej Karpathy's video ["Let's Build GPT"](https://youtu.be/kCc8FmEb1nY?si=_XJjWSxI8lkCnmXG), with modifications to work on a custom dataset: **Harry Potter**. The repository includes both a **Bigram Model** and a **Decoder-Only Transformer**.
The files `./EX1.ipynb` and `./EX1(2).ipynb` are my solutions to the challenges stated in the video description. 

## Features

- **Bigram Model**: A simpler language model that forms the basis of understanding token relationships.
- **Decoder-Only Transformer**: A more advanced model that builds on the Bigram Model to generate text.

## Output

The output generated by the Transformer model can be found in `./output.txt`. While the text may not be entirely coherent due to the limited dataset and training constraints, it demonstrates the model's ability to generate text-like sequences rather than random tokens.

## Purpose

The purpose of this project was to experiment further with the code and ensure that I truly understood the concepts presented in the video, by applying these ideas to a different dataset.
